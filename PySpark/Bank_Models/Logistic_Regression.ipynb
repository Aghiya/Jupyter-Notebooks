{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Session and read CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Bank Data-Logistic Regression').getOrCreate()\n",
    "df = spark.read.csv('./bankdata1.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null column at end of data.\n",
    "df = df.drop('_c16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use StringIndexer on categorical variable columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in categorical_features]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_r = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original columns.\n",
    "df_r = df_r.drop('job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use VectorAssembler to compress all independent variable columns into dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of IVs.\n",
    "feat_cols = ['age', 'balance','day', 'campaign', 'pdays', 'previous', 'job_index', 'marital_index', 'education_index', 'default_index', 'housing_index', 'loan_index', 'contact_index', 'month_index', 'poutcome_index']\n",
    "\n",
    "# Create the vector assembler and name the vector column \"features\".\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol='features')\n",
    "\n",
    "# Create the new dataframe that has the \"features\" column of dense vectors..\n",
    "dense_df = vec_assembler.transform(df_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final pre-processed dataframe consisting only of IV vectors and the churn indicator.\n",
    "final_df = dense_df.select('features', 'y_index')\n",
    "\n",
    "# Split the data into three categories: training data, testing data, and validation data.\n",
    "train, test, valid = final_df.randomSplit([.6, .2, .2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression model, fit the model to the training data.\n",
    "lr = LogisticRegression(labelCol='y_index')\n",
    "lr = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data prediction accuracy: 0.890\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the testing data and print the accuracy.\n",
    "test_preds = lr.transform(test)\n",
    "print(\"Test data prediction accuracy: {0:.3f}\".format(test_preds.filter(test_preds.y_index == test_preds.prediction).count() / test_preds.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data prediction accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the validation data and print the accuracy.\n",
    "valid_preds = lr.transform(valid)\n",
    "print(\"Validation data prediction accuracy: {0:.3f}\".format(valid_preds.filter(valid_preds.y_index == valid_preds.prediction).count() / valid_preds.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get metrics via BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the probabilities and churn indicator into a list.\n",
    "results = test_preds.select(['probability', 'y_index'])\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test ROC score is:  0.7375987905514824\n"
     ]
    }
   ],
   "source": [
    "# Convert results_list into an RDD.\n",
    "scoreAndLabels = spark.sparkContext.parallelize(results_list)\n",
    "\n",
    "# Use BinaryClassificationMetrics on the results\n",
    "metrics = metric(scoreAndLabels)\n",
    "\n",
    "print(\"The test ROC score is: \", metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 7872 97 891\n"
     ]
    }
   ],
   "source": [
    "# Get confusion matrix values\n",
    "tp = test_preds.filter((test_preds.y_index == 1) & (test_preds.prediction == 1)).count()\n",
    "tn = test_preds.filter((test_preds.y_index == 0) & (test_preds.prediction == 0)).count()\n",
    "fp = test_preds.filter((test_preds.y_index == 0) & (test_preds.prediction == 1)).count()\n",
    "fn = test_preds.filter((test_preds.y_index == 1) & (test_preds.prediction == 0)).count()\n",
    "print(tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
