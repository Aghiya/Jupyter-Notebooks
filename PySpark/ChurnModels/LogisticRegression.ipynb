{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import  OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.sql import functions as F, SparkSession\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSession and read CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Telco Data - Logistic Regression').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./telcochurn.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop customer ID column as it is not useful for training a model.\n",
    "df = df.drop('customerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary columns of \"Yes/No\" to \"1/0\".\n",
    "df = df.withColumn('Partner_2', F.when(df.Partner == 'Yes', 1).otherwise(0))\n",
    "df = df.withColumn('Dependents_2', F.when(df.Dependents == 'Yes', 1).otherwise(0))\n",
    "df = df.withColumn('PhoneService_2', F.when(df.PhoneService == 'Yes', 1).otherwise(0))\n",
    "df = df.withColumn('PaperlessBilling_2', F.when(df.PaperlessBilling == 'Yes', 1).otherwise(0))\n",
    "df = df.withColumn('Churn_2', F.when(df.Churn == 'Yes', 1).otherwise(0))\n",
    "\n",
    "# Convert TotalCharges from strings to doubles.\n",
    "df = df.withColumn('TotalCharges_2', df.TotalCharges.cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old binary columns.\n",
    "df = df.drop('Partner', 'Dependents', 'PhoneService', 'Paperlessbilling', 'Churn', 'TotalCharges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use StringIndexer on categorical variable columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that are categorical.\n",
    "categorical_features = ['gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "                        'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of indexers to apply to the data.\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and apply the indexers.\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "indexed_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original columns.\n",
    "indexed_df = indexed_df.drop('gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup','DeviceProtection', \n",
    "                             'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sparse vectors of each indexed column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns that are indexed.\n",
    "indexed_cols = ['MultipleLines_index', 'InternetService_index', 'OnlineSecurity_index', 'OnlineBackup_index', \n",
    "                'DeviceProtection_index', 'TechSupport_index', 'StreamingTV_index', 'StreamingMovies_index', \n",
    "                'Contract_index', 'PaymentMethod_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder.\n",
    "encoder = OneHotEncoderEstimator(inputCols=indexed_cols,\n",
    "                                 outputCols=[feat + '_sparse' for feat in indexed_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and apply encoder to dataframe.\n",
    "model = encoder.fit(indexed_df)\n",
    "sparse_indexed_df = model.transform(indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-sparse indexed columns.\n",
    "sparse_indexed_df = sparse_indexed_df.drop('MultipleLines_index', 'InternetService_index', 'OnlineSecurity_index',\n",
    "                                           'OnlineBackup_index', 'DeviceProtection_index', 'TechSupport_index', \n",
    "                                           'StreamingTV_index', 'StreamingMovies_index', 'Contract_index', \n",
    "                                           'PaymentMethod_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use VectorAssembler to compress all independent variable columns into a dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of IVs.\n",
    "IV_features = ['SeniorCitizen', 'tenure', 'Paperlessb.illing_2', 'MonthlyCharges', 'TotalCharges_2', 'Partner_2', \n",
    "               'Dependents_2', 'PhoneService_2', 'gender_index', 'StreamingTV_index_sparse', 'TechSupport_index_sparse', \n",
    "               'PaymentMethod_index_sparse', 'OnlineSecurity_index_sparse', 'InternetService_index_sparse', \n",
    "               'StreamingMovies_index_sparse', 'MultipleLines_index_sparse', 'Contract_index_sparse',\n",
    "               'OnlineBackup_index_sparse', 'DeviceProtection_index_sparse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector assembler and name the vector column \"features\". The handleInvalid parameter is set to drop rows that \n",
    "# contain NA values. In this case, a total of 11 rows are dropped.\n",
    "vec_assembler = VectorAssembler(inputCols=IV_features, outputCol='features', handleInvalid='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"Paperlessb.illing_2\" does not exist.\\nAvailable fields: SeniorCitizen, tenure, MonthlyCharges, Partner_2, Dependents_2, PhoneService_2, PaperlessBilling_2, Churn_2, TotalCharges_2, gender_index, StreamingTV_index_sparse, TechSupport_index_sparse, PaymentMethod_index_sparse, OnlineSecurity_index_sparse, InternetService_index_sparse, StreamingMovies_index_sparse, MultipleLines_index_sparse, Contract_index_sparse, OnlineBackup_index_sparse, DeviceProtection_index_sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o533.transform.\n: java.lang.IllegalArgumentException: Field \"Paperlessb.illing_2\" does not exist.\nAvailable fields: SeniorCitizen, tenure, MonthlyCharges, Partner_2, Dependents_2, PhoneService_2, PaperlessBilling_2, Churn_2, TotalCharges_2, gender_index, StreamingTV_index_sparse, TechSupport_index_sparse, PaymentMethod_index_sparse, OnlineSecurity_index_sparse, InternetService_index_sparse, StreamingMovies_index_sparse, MultipleLines_index_sparse, Contract_index_sparse, OnlineBackup_index_sparse, DeviceProtection_index_sparse\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\r\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:162)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:161)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:161)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dcfad8f884df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the new dataframe that has the \"features\" column of dense vectors..\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdense_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvec_assembler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_indexed_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'Field \"Paperlessb.illing_2\" does not exist.\\nAvailable fields: SeniorCitizen, tenure, MonthlyCharges, Partner_2, Dependents_2, PhoneService_2, PaperlessBilling_2, Churn_2, TotalCharges_2, gender_index, StreamingTV_index_sparse, TechSupport_index_sparse, PaymentMethod_index_sparse, OnlineSecurity_index_sparse, InternetService_index_sparse, StreamingMovies_index_sparse, MultipleLines_index_sparse, Contract_index_sparse, OnlineBackup_index_sparse, DeviceProtection_index_sparse'"
     ]
    }
   ],
   "source": [
    "# Create the new dataframe that has the \"features\" column of dense vectors..\n",
    "dense_df = vec_assembler.transform(sparse_indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe consisting only of IV vector and the churn indicator.\n",
    "processed_df = dense_df.select('features', 'Churn_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into three categories: training data, testing data, and validation data.\n",
    "train, test, valid = processed_df.randomSplit([.6, .2, .2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create, train, and predict with Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression model, fit the model to the training data.\n",
    "model = LogisticRegression(labelCol='Churn_2')\n",
    "model = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the testing data and print the accuracy.\n",
    "test_preds = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the validation data and print the accuracy.\n",
    "valid_preds = model.transform(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_score(preds_df):\n",
    "    return preds_df.filter(preds_df.Churn_2 == preds_df.prediction).count() / preds_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = get_accuracy_score(test_preds)\n",
    "valid_accuracy = get_accuracy_score(valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test data prediction accuracy: {0:.3f}\".format(test_accuracy))\n",
    "print(\"Validation data prediction accuracy: {0:.3f}\".format(valid_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrix values.\n",
    "test_tp = test_preds.filter((test_preds.Churn_2 == 1) & (test_preds.prediction == 1)).count()\n",
    "test_tn = test_preds.filter((test_preds.Churn_2 == 0) & (test_preds.prediction == 0)).count()\n",
    "test_fp = test_preds.filter((test_preds.Churn_2 == 0) & (test_preds.prediction == 1)).count()\n",
    "test_fn = test_preds.filter((test_preds.Churn_2 == 1) & (test_preds.prediction == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix.\n",
    "test_cm =[[test_tn, test_fp], [test_fn, test_tp]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confusion matrix values.\n",
    "valid_tp = valid_preds.filter((valid_preds.Churn_2 == 1) & (valid_preds.prediction == 1)).count()\n",
    "valid_tn = valid_preds.filter((valid_preds.Churn_2 == 0) & (valid_preds.prediction == 0)).count()\n",
    "valid_fp = valid_preds.filter((valid_preds.Churn_2 == 0) & (valid_preds.prediction == 1)).count()\n",
    "valid_fn = valid_preds.filter((valid_preds.Churn_2 == 1) & (valid_preds.prediction == 0)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix.\n",
    "valid_cm =[[valid_tn, valid_fp], [valid_fn, valid_tp]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot with Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tickmark names.\n",
    "classNames = ['Negative','Positive']\n",
    "\n",
    "# Set the text scale for Seaborn.\n",
    "sb.set(font_scale=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set each figure's size.\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "#----------\n",
    "# Test Dataset.\n",
    "#----------\n",
    "\n",
    "# Create first subplot.\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "# Set title.\n",
    "plt.title('Test Dataset')\n",
    "\n",
    "# Create confusion matrix as a Seaborn heatmap.\n",
    "sb.heatmap(test_cm, annot=np.array(test_cm), fmt='d', cbar=False, xticklabels=classNames, yticklabels=classNames, square=True)\n",
    "\n",
    "# Set y tick-label rotation angle.\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Set x and y axes names.\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "#----------\n",
    "# Validation Dataset.\n",
    "#----------\n",
    "\n",
    "# Create second subplot.\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "# Set title.\n",
    "plt.title('Validation Dataset')\n",
    "\n",
    "# Create confusion matrix as a Seaborn heatmap.\n",
    "sb.heatmap(valid_cm, annot=np.array(valid_cm), fmt='d', cbar=False, xticklabels=classNames, yticklabels=classNames, square=True)\n",
    "\n",
    "# Set y tick-label rotation angle.\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Set x axis name.\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Adjust distance from the right edge of each subplot.\n",
    "plt.subplots_adjust(right=1.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate FPR, TPR, and AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of 'Churn_2' column values.\n",
    "test_churn_list = test_preds.select('Churn_2').collect()\n",
    "test_churn_list = [test_churn_list[i][0] for i in range(len(test_churn_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of 'prediction' column values.\n",
    "test_pred_list = test_preds.select('prediction').collect()\n",
    "test_pred_list = [test_pred_list[i][0] for i in range(len(test_pred_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FPR, TPR.\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(test_churn_list, test_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AUC.\n",
    "test_auc = auc(test_fpr, test_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of 'Churn_2' column values.\n",
    "valid_churn_list = valid_preds.select('Churn_2').collect()\n",
    "valid_churn_list = [valid_churn_list[i][0] for i in range(len(valid_churn_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of 'prediction' column values.\n",
    "valid_pred_list = valid_preds.select('prediction').collect()\n",
    "valid_pred_list = [valid_pred_list[i][0] for i in range(len(valid_pred_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FPR, TPR.\n",
    "valid_fpr, valid_tpr, valid_thresholds = roc_curve(valid_churn_list, valid_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AUC.\n",
    "valid_auc = auc(valid_fpr, valid_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear plots.\n",
    "plt.clf()\n",
    "\n",
    "# Set plot and text sizes.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.rcParams.update({'font.size': 13, 'xtick.labelsize' : 18, 'ytick.labelsize' : 18})\n",
    "\n",
    "#============\n",
    "# Test ROC curve\n",
    "#============\n",
    "\n",
    "# Create first subplot with a white background.\n",
    "plt.subplot(1,2,1, fc='white')\n",
    "\n",
    "# ax = plt.axes()\n",
    "# ax.set_facecolor('white')\n",
    "\n",
    "# Set title.\n",
    "plt.title('Test Dataset')\n",
    "\n",
    "# Plot the data, fpr = x, tpr = y. Bold, with a label for the AUC.\n",
    "plt.plot(test_fpr, test_tpr, 'b', label = 'AUC = %0.3f' % test_auc)\n",
    "\n",
    "# Place legend on plot.\n",
    "plt.legend(loc = 'lower right')\n",
    "\n",
    "# Plot a dotted red straight line for comparison\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "# Set x and y boundaries.\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Label axes.\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "#============\n",
    "# Validation ROC curve\n",
    "#============\n",
    "\n",
    "# Create second subplot with a white background.\n",
    "plt.subplot(1,2,2, fc='white')\n",
    "\n",
    "# Set title.\n",
    "plt.title('Validation Dataset')\n",
    "\n",
    "# Plot the data, fpr = x, tpr = y. Bold, with a label for the AUC.\n",
    "plt.plot(valid_fpr, valid_tpr, 'b', label = 'AUC = %0.3f' % valid_auc)\n",
    "\n",
    "# Place legend on plot.\n",
    "plt.legend(loc = 'lower right')\n",
    "\n",
    "# Plot a dotted red straight line for comparison\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "# Set x and y boundaries\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Label x axis.\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "# Adjust distance from the right edge of each subplot.\n",
    "plt.subplots_adjust(right=1.3)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
